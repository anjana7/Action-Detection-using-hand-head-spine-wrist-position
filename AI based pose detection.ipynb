{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def optimize_floats(dataframe):\n",
    "    float_col = dataframe.select_dtypes(include=['float64']).columns.tolist()\n",
    "    dataframe[float_col] = dataframe[float_col].apply(pd.to_numeric, downcast='float')\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def read_data(path=\"dataset/train.csv\"):\n",
    "    \n",
    "    '''\n",
    "    Reading the data and optimizing the column dtypes to \n",
    "    minimal suitable dtype for memory efficiency.\n",
    "    Dropping the Recording time column.\n",
    "    '''\n",
    "    data_orig = pd.read_csv(path)\n",
    "    data_orig = optimize_floats(data_orig)\n",
    "    data = data_orig.drop([\"Recording_time_ID\"], axis=1)\n",
    "    return(data, data_orig)\n",
    "\n",
    "\n",
    "def label_encode(to_encode, train, inv=False):\n",
    "    \n",
    "    '''\n",
    "    Action column is string eg. Response-1.\n",
    "    Encoding the column values to integers.\n",
    "    '''\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(train[[\"Action\"]].values)\n",
    "    \n",
    "    if not inv:\n",
    "        to_encode[[\"Action\"]] = label_encoder.transform(to_encode[[\"Action\"]].values)\n",
    "        to_encode[[\"Action\"]] = to_encode[[\"Action\"]].astype('int')\n",
    "        return(to_encode)\n",
    "    else:\n",
    "        encoded = label_encoder.inverse_transform(to_encode)\n",
    "        return(encoded)\n",
    "\n",
    "\n",
    "def feature_creation(data, c1, c2):\n",
    "    '''\n",
    "    Calculating the similarity between the columns \n",
    "    like left hand and right hand movement. \n",
    "    '''\n",
    "    \n",
    "    df_similarity = pd.DataFrame(\n",
    "        np.diagonal(\n",
    "            cosine_similarity(\n",
    "                data[c1], \n",
    "                data[c2],\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return(df_similarity)\n",
    "    \n",
    "def feature_generator(data):\n",
    "    \n",
    "    '''\n",
    "    Generating the extra features using the probable related columns.\n",
    "    '''\n",
    "    col_dict = {\n",
    "        'df_lh_rh':[['lhx', 'lhy', 'lhz'], ['rhx', 'rhy', 'rhz']],\n",
    "        'df_hd_lh':[['hx', 'hy', 'hz'], ['lhx', 'lhy', 'lhz']], \n",
    "        'df_hd_rh':[['hx', 'hy', 'hz'], ['rhx', 'rhy', 'rhz']], \n",
    "        'df_hd_sp':[['hx', 'hy', 'hz'], ['sx', 'sy', 'sz']], \n",
    "        'df_lh_lw':[['lhx', 'lhy', 'lhz'], ['lwx', 'lwy', 'lwz']], \n",
    "        'df_rh_rw':[['rhx', 'rhy', 'rhz'], ['rwx', 'rwy', 'rwz']], \n",
    "        'df_vlh_vrh':[['vlhx', 'vlhy', 'vlhz'], ['vrhx', 'vrhy', 'vrhz']], \n",
    "        'df_vlw_vrw':[['vlwx', 'vlwy', 'vlwz'], ['vrwx', 'vrwy', 'vrwz']], \n",
    "        'df_alh_arh':[['alhx', 'alhy', 'alhz'], ['arhx', 'arhy', 'arhz']], \n",
    "        'df_alw_arw':[['alwx', 'alwy', 'alwz'], ['arwx', 'arwy', 'arwz']]\n",
    "    }\n",
    "    \n",
    "    data_appended = data\n",
    "    for k,v in col_dict.items():\n",
    "        df = feature_creation(data, v[0], v[1]).rename(columns={0:k})\n",
    "        data_appended = pd.concat([df, data_appended], axis=1)\n",
    "    \n",
    "    return(data_appended)\n",
    "        \n",
    "\n",
    "def feature_scaling(X_train, x):\n",
    "    \n",
    "    '''\n",
    "    Normalising and Scaling the features for better model training.\n",
    "    '''\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(X_train)\n",
    "    X_scaled = sc.transform(x)\n",
    "    return(X_scaled)\n",
    "    \n",
    "def refit_al(test_data, data_label_encoded, model, thres=0.9):\n",
    "    \n",
    "    '''\n",
    "    Since the dataset is a little imbalanced \n",
    "    Active Learning can be used to find the predictions \n",
    "    for the test data points. The data points where the model \n",
    "    predicts the class with more than thres=90% probability \n",
    "    can be considered to be true class.\n",
    "    Further since the data points for Response-2, Response-4 \n",
    "    and Response-5 are low, only those particular data points \n",
    "    can be added and model can be retrained.\n",
    "    '''\n",
    "    preds = model.predict(test_data)\n",
    "    max_prob = np.amax(model.predict_proba(test_data), axis=1)\n",
    "    res_ind = np.nonzero(np.where(max_prob > thres, max_prob, 0))\n",
    "    \n",
    "    action = pd.DataFrame(preds[res_ind[0]]).rename(columns={0:'Action'})\n",
    "    df_confident = pd.DataFrame(test_data.iloc[res_ind].values).rename(columns=dict(enumerate(test_data.columns)))\n",
    "    df_confident = pd.concat([df_confident, action], axis=1)\n",
    "    \n",
    "    df_confident_1 = df_confident[df_confident['Action']==1]\n",
    "    df_confident_3 = df_confident[df_confident['Action']==3]\n",
    "    df_confident_4 = df_confident[df_confident['Action']==4]\n",
    "    \n",
    "    train_data_new = pd.concat([data_label_encoded, df_confident_1, df_confident_3, df_confident_4], axis=0)\n",
    "    \n",
    "    y = np.ravel(train_data_new[['Action']].values)\n",
    "    X = train_data_new.drop(['Action'], axis=1)\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    return(model)\n",
    "    \n",
    "def parameter_tuning(X_train, y_train):\n",
    "    x_train = feature_scaling(X_train, X_train)\n",
    "    n_estimators = [int(x) for x in linspace(start=100, stop=2000, num=10)]\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    max_depth = [int(x) for x in linspace(10, 110, num=11)]\n",
    "    max_depth.append(None)\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    bootstrap = [True, False]\n",
    "    \n",
    "    random_grid = {'n_estimators' : n_estimators, 'max_features' : max_features, 'max_depth' : max_depth, 'min_samples_split':min_samples_split, 'min_samples_leaf' : min_samples_leaf, 'bootstrap': bootstrap}\n",
    "    rf = RandomForestClassifier()\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1).fit(x_train, y_train)\n",
    "    return(rf_random)\n",
    "\n",
    "def model_fitting(X, y, smote=False):\n",
    "    if smote==True:\n",
    "        X_train, y_train = smote_data_generation(X, y)\n",
    "        rf_random = parameter_tuning(X_train, y_train)\n",
    "        model = rf_random.best_estimator_\n",
    "    else:\n",
    "        model = RandomForestClassifier(n_estimators=500, min_samples_split=2, max_depth=40, min_samples_leaf=1, bootstrap=False).fit(X, y)\n",
    "        model.fit(X_train, y_train)\n",
    "    return(model)\n",
    "\n",
    "\n",
    "def traini\n",
    "data, data_orig = read_data()\n",
    "data_label_encoded = label_encode(to_encode=data, train=data_orig)\n",
    "data_feature_extracted = feature_generation(data_label_encoded)\n",
    "\n",
    "y = data_label_encoded[['Action']].values\n",
    "X = data_label_encoded.drop(['Action'], axis=1)\n",
    "\n",
    "model = model_fitting(X, y, smote=False)\n",
    "\n",
    "'''\n",
    "In case the smote is not used we can get good test results \n",
    "by using Active Learning. So here only incase of not using smote\n",
    "we are using AL as only lower frequent classes are being added \n",
    "in the train set.\n",
    "'''\n",
    "\n",
    "test_data, test_orig = read_data('dataset/test.csv')\n",
    "i = 0\n",
    "while i < 4:\n",
    "    model = refit_al(test_data, data_label_encoded, model, thres=0.96) \n",
    "    i+=1\n",
    "\n",
    "predictions = model.predict(test_data)\n",
    "predicted_actions = label_encode(to_encode=predictions, train=data_orig, inv=True)\n",
    "df_pred = pd.DataFrame(data=predicted_actions).rename(columns={0:'Action'})\n",
    "\n",
    "pd.concat([test_orig[['Recording_time_ID']], df_pred], axis=1).to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
